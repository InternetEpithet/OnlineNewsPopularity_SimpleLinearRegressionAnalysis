---
title: "STAC67 Final Project"
author: "Alex Cheng, Zaamin Rattansi, Jacob Temple, Jeffrey Wong"
date: "12/05/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
library(ggplot2)
library(GGally)
library(MASS)
library(leaps)

library(MPV)
library(car)
```

# Introduction
The purpose of this research is to study the direct and indirect relationships of metrics for Mashable news articles and the amount an article was shared by readers. This will be done through thorough analysis of the variables that have an effect on the number of shares of an article. Variables that are directly related, and variables that have less importance were included to see the largest range of relationships possible and to really understand what affects the amount an article is shared by Mashable users. 


## Background
The data we have analyzed and studied is taken from a multi-platform media company called Mashable. The data was taken to analyze the number of shares a new article recieves given other variables. The chosen variables had clear relations to the amount a post may be shared and our models show the importance of some factors vs others regarding the amount a post may be shared. There are many factors that cause a post to shared heavily, 

## Study Goal
We will analyze what variables cause an increase or decrease in the number of shares to learn more about what future articles require in order to be shared more by Mashable users. 

## Description of Dataset

The data was cleaned mainly be eliminating variables that were either redundant to the number of shares or if similar variables were used. For example, we removed any max and min variables and instead used the average variables. It was unnecessary to use all three, the max, min, and average, when we can just use the average variables. We also removed all the LDA variables since the dummy variables for data channel described what the LDA variables would be in a simpler way. Including these variables would also introduce multicollinearity. 



```{r echo=FALSE}
# Set seed for reproducibility of data split
set.seed(123456)

# Load data set into data frame
data_set <- read.csv("OnlineNewsPopularity.csv")

# Consider only a subset of the explanatory variables by removing the unused ones
# Since the data set comes with the categorical variables already factored, must remove baseline variables

#shares <- subset(data_set, select = -c(1,2,7,19,20,21,23,24,26,27,29,30,38,39,40,41,42,43,44,49,50,52,53,55,56))
#do not remove 38-44. do not remove 19
shares <- subset(data_set, select = -c(1,2,19,20,21,23,24,26,27,38, 39, 40, 41, 42, 43, 44, 49,50, 53))

# Randomly split data set ~50/50 for building/validating
split <- rbinom(nrow(shares), 1, 0.5)

shares_build <- shares[split == 1, ]
shares_valid <- shares[split == 0, ]
```

```{r include=FALSE}
weekday = c('Mon', 'Tues', 'Wed', 'Thur', 'Fri', 'Sat', 'Sun')
count_weekday = c(6661, 7390, 7435, 7262, 5701, 2453, 2737)
data_channel = c('Lifestyle', 'Entertain', 'Bus', 'Socmed', 'Tech', 'World')
count_channel = c(2099, 7057, 6258, 2323, 7346, 8427)
```

```{r echo=FALSE}
barplot(count_weekday,names.arg=weekday, xlab="Weekday",ylab="count",col="red", space = 2)
```

```{r echo=FALSE}
barplot(count_channel,names.arg=data_channel, xlab="Data Channel",ylab="count",col="red",space = 2)
```

```{r echo=FALSE}
data_set <- read.csv("OnlineNewsPopularity.csv")
num_vars <- subset(data_set, select = c(8, 10, 11, 13))
boxplot(num_vars)
```


# Model Building

## Full Model
```{r}
# Build full model with interactions
fit0 <- lm(shares ~ . +
             n_tokens_content:(weekday_is_monday + weekday_is_tuesday +
                                    weekday_is_wednesday + weekday_is_thursday +
                                    weekday_is_friday + weekday_is_saturday) +
             global_subjectivity:(avg_negative_polarity + abs_title_subjectivity +
                                    abs_title_sentiment_polarity) +
             num_imgs:num_hrefs
            + weekday_is_monday:(data_channel_is_bus +data_channel_is_tech)
            + weekday_is_wednesday:(data_channel_is_tech )
            + weekday_is_friday:(data_channel_is_entertainment)
            + num_self_hrefs:num_hrefs + num_imgs:average_token_length + weekday_is_monday:(avg_negative_polarity+avg_positive_polarity)
            +weekday_is_tuesday:(avg_negative_polarity)
            +weekday_is_wednesday:(avg_negative_polarity+avg_positive_polarity)
            + data_channel_is_entertainment:(avg_positive_polarity+avg_negative_polarity)
            +avg_positive_polarity:(data_channel_is_socmed+data_channel_is_lifestyle)
            +avg_negative_polarity:(data_channel_is_bus)
            +abs_title_sentiment_polarity:(data_channel_is_lifestyle+data_channel_is_entertainment ) + n_tokens_content:avg_negative_polarity
            +abs_title_subjectivity:(data_channel_is_bus+data_channel_is_tech )
            +title_sentiment_polarity:(data_channel_is_entertainment+ data_channel_is_bus)
            +n_non_stop_words:(kw_avg_max)
            +n_tokens_content:(kw_avg_max+kw_avg_min+kw_avg_avg)
            + n_non_stop_unique_tokens:(kw_avg_max+kw_avg_avg)
            +average_token_length:(kw_avg_max+kw_avg_min)
            +num_keywords:(kw_avg_max+kw_avg_min)
            +kw_avg_avg:(global_rate_positive_words+global_sentiment_polarity)
            +global_rate_negative_words:(kw_avg_max)
            +global_sentiment_polarity:(weekday_is_tuesday+weekday_is_saturday)
            

            +self_reference_avg_sharess:(weekday_is_monday+weekday_is_tuesday+weekday_is_thursday+weekday_is_saturday)
            +self_reference_avg_sharess:(data_channel_is_entertainment+data_channel_is_bus+data_channel_is_socmed+data_channel_is_tech)
            +self_reference_avg_sharess:(title_subjectivity+abs_title_subjectivity)
            +self_reference_avg_sharess:(global_subjectivity+global_sentiment_polarity+global_rate_positive_words)
            +self_reference_avg_sharess:(kw_avg_max+kw_avg_avg)
            +self_reference_avg_sharess:(n_tokens_content+n_unique_tokens+num_keywords)
            +self_reference_avg_sharess:(num_self_hrefs+num_hrefs+num_videos)
            +self_reference_avg_sharess:(self_reference_max_shares+self_reference_min_shares)
            
            +self_reference_max_shares:(weekday_is_monday+weekday_is_wednesday+weekday_is_thursday+weekday_is_friday)
            +self_reference_max_shares:(global_subjectivity+global_sentiment_polarity+global_rate_positive_words)
            
            +avg_negative_polarity:(min_negative_polarity+max_negative_polarity)
            
            +num_videos:(data_channel_is_tech)
            +num_imgs:( data_channel_is_bus + data_channel_is_socmed)
            , data=shares_build)
summary(fit0)
```

## Model 1
```{r include=FALSE}
# Stepwise regression (AIC based)
fit_simple <- lm(shares ~ 1, data=shares_build)
step <- stepAIC(fit_simple, direction="both", scope=list(upper=fit0,lower=fit_simple))

# Best model based on stepwise regression
vars1 <- paste(attr(terms(step), "term.labels"), collapse = "+")
fit1 <- lm(paste("shares ~", vars1, sep = " "), data=shares_build)

summary(fit1)
```

## Model 2
```{r include=FALSE}
# All possible (subset) regressions on fit1

# To see all subsets
# options(max.print=50000)

# calculate all subsets of <=13 predictors' diagnostics
all_shares <- regsubsets(as.formula(paste("shares ~", vars1, sep = " ")),
                         nbest = 6000, really.big=T, nvmax=32, data = shares_build)
aprout <- summary(all_shares)
n <- nrow(shares_build)
p <- apply(aprout$which, 1, sum)
aprout$aic <- aprout$bic - log(n)*p+2*p
with(aprout, round(cbind(which,rsq,adjr2,cp,bic,aic),3))

#Find the best model
best_aic <- which.min(aprout[["aic"]])
best_bic <- which.min(aprout[["bic"]])
best_rsq <- which.max(aprout[["rsq"]])
best_adjr2 <- which.max(aprout[["adjr2"]])
best_cp <- which.min(aprout[["cp"]])
stats <- cbind(best_aic, best_bic, best_rsq, best_adjr2, best_cp)

# Mode function
getmode <- function(v) {
 uniqv <- unique(v)
 uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Best model based on rsq, adjr2, cp, bic, and aic
vars2 <- paste(attr(terms(step), "term.labels")[as.numeric(aprout[["which"]][getmode(stat),2:36 ]) == 1], collapse="+")
fit2 <- lm(paste("shares ~", vars2, sep = " "), data=shares_build)

summary(fit2)
```



# Model Diagnostics



```{r}
t = rstudent(fit2)

alpha = 0.05
n = dim(shares_build)[1]
p.prime = length(coef(fit2))

t.crit = qt(1-alpha/(2*n), n-p.prime-1)

#return index of vector satisfying the condition
Outlier <- which(abs(t) > t.crit)
Outlier
```

```{r}
hii = hatvalues(fit2)
which(hii>2*p.prime/n)
Leverage <- which(hii>0.5)
```

```{r}
DFFITS = dffits(fit2)
Influence1 <- c(which(abs(DFFITS) > 2*sqrt(p.prime/n)))
Influence1

DFBETAS = dfbetas(fit2)
Influence2 <- c(which(abs(DFBETAS) > 2*sqrt(1/n)))
Influence2
```


```{r}
removal = unique(c(Outlier, Leverage, Influence1, Influence2))

new_build = shares_build[-c(removal),]

```

```{r}
Influence3 = unique(c(Influence1, Influence2))
Significant = unique(c(Outlier, Leverage))

remove = intersect(Influence3, Significant)

new_new_build = shares_build[-c(remove)]
```


## Model 3
```{r}
#normal and constant variance check for fit0
resid = fit0$residuals
pred = fit0$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```
  

```{r}
result <- boxcox(fit0)
result
mylambda = result$x[which.max(result$y)]
mylambda

y.star = log(new_build$shares)
fit3 = lm(log(shares) ~ . +
             n_tokens_content:(weekday_is_monday + weekday_is_tuesday +
                                    weekday_is_wednesday + weekday_is_thursday +
                                    weekday_is_friday + weekday_is_saturday) +
             global_subjectivity:(avg_negative_polarity + abs_title_subjectivity +
                                    abs_title_sentiment_polarity) +
             num_imgs:num_hrefs
            + weekday_is_monday:(data_channel_is_bus +data_channel_is_tech)
            + weekday_is_wednesday:(data_channel_is_tech )
            + weekday_is_friday:(data_channel_is_entertainment)
            + num_self_hrefs:num_hrefs + num_imgs:average_token_length + weekday_is_monday:(avg_negative_polarity+avg_positive_polarity)
            +weekday_is_tuesday:(avg_negative_polarity)
            +weekday_is_wednesday:(avg_negative_polarity+avg_positive_polarity)
            + data_channel_is_entertainment:(avg_positive_polarity+avg_negative_polarity)
            +avg_positive_polarity:(data_channel_is_socmed+data_channel_is_lifestyle)
            +avg_negative_polarity:(data_channel_is_bus)
            +abs_title_sentiment_polarity:(data_channel_is_lifestyle+data_channel_is_entertainment ) + n_tokens_content:avg_negative_polarity
            +abs_title_subjectivity:(data_channel_is_bus+data_channel_is_tech )
            +title_sentiment_polarity:(data_channel_is_entertainment+ data_channel_is_bus)
            +n_non_stop_words:(kw_avg_max)
            +n_tokens_content:(kw_avg_max+kw_avg_min+kw_avg_avg)
            + n_non_stop_unique_tokens:(kw_avg_max+kw_avg_avg)
            +average_token_length:(kw_avg_max+kw_avg_min)
            +num_keywords:(kw_avg_max+kw_avg_min)
            +kw_avg_avg:(global_rate_positive_words+global_sentiment_polarity)
            +global_rate_negative_words:(kw_avg_max)
            +global_sentiment_polarity:(weekday_is_tuesday+weekday_is_saturday)
            +self_reference_avg_sharess:(weekday_is_monday+weekday_is_tuesday+weekday_is_thursday+weekday_is_saturday)
            +self_reference_avg_sharess:(data_channel_is_entertainment+data_channel_is_bus+data_channel_is_socmed+data_channel_is_tech)
            +self_reference_avg_sharess:(title_subjectivity+abs_title_subjectivity)
            +self_reference_avg_sharess:(global_subjectivity+global_sentiment_polarity+global_rate_positive_words)
            +self_reference_avg_sharess:(kw_avg_max+kw_avg_avg)
            +self_reference_avg_sharess:(n_tokens_content+n_unique_tokens+num_keywords)
            +self_reference_avg_sharess:(num_self_hrefs+num_hrefs+num_videos)
            +self_reference_avg_sharess:(self_reference_max_shares+self_reference_min_shares)
            
            +self_reference_max_shares:(weekday_is_monday+weekday_is_wednesday+weekday_is_thursday+weekday_is_friday)
            +self_reference_max_shares:(global_subjectivity+global_sentiment_polarity+global_rate_positive_words)
            
            +avg_negative_polarity:(min_negative_polarity+max_negative_polarity)
            +num_videos:(data_channel_is_tech)
            +num_imgs:( data_channel_is_bus + data_channel_is_socmed)
            , data=new_build)

resid = fit3$residuals
pred = fit3$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```

## Model 4
```{r}
#normal and constant variance check for fit1
resid = fit1$residuals
pred = fit1$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```

```{r}
result <- boxcox(fit1)
result
mylambda = result$x[which.max(result$y)]
mylambda

y.star = log(new_build$shares)
fit4 = lm(paste("y.star ~", vars1, sep = " "), data=new_build)

resid = fit4$residuals
pred = fit4$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```

## Model 5
```{r}
#normal and constant variance check for fit2
resid = fit2$residuals
pred = fit2$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```

```{r}
result <- boxcox(fit2)
result
mylambda = result$x[which.max(result$y)]
mylambda

y.star = log(new_build$shares)
fit5 = lm(paste("y.star ~", vars2, sep = " "), data=new_build)

resid = fit5$residuals
pred = fit5$fitted.values

par(mfrow=c(1,2))
plot(pred, resid, pch=20, col="red",cex=2)
abline(c(0,0))

qqnorm(resid)
qqline(resid)
```


```{r}
PRESS(fit0)
PRESS(fit1)
PRESS(fit2)
PRESS(fit3)
PRESS(fit4)
PRESS(fit5)
```

```{r}
summary(fit3)
summary(fit4)
summary(fit5)
```


```{r}
VIF0 = vif(fit0)

VIF1 = vif(fit1)

VIF2 = vif(fit2)

VIF3 = vif(fit3)
VIF3

VIF4 = vif(fit4)
VIF4

VIF5 = vif(fit5)
VIF5
```


```{r include=FALSE}
# Remove insignificant variables
f_test <- drop1(fit5, test = "F")

fit6 <- lm(shares ~ average_token_length:num_imgs
           + average_token_length:kw_avg_min
           + weekday_is_monday:self_reference_max_shares
           + self_reference_max_shares:global_subjectivity
           + weekday_is_monday:self_reference_avg_sharess
           + self_reference_avg_sharess:global_subjectivity 
           + kw_avg_avg:self_reference_avg_sharess
           + self_reference_min_shares:self_reference_avg_sharess
           + self_reference_avg_sharess:kw_avg_max
           + self_reference_avg_sharess:num_keywords,
           data = shares_build)

summary(fit6)
```

# Model Validation

```{r}

predict_value = predict(fit3, shares_valid)
predict_value1 = predict(fit4, shares_valid)
predict_value2 = predict(fit5, shares_valid)

n = dim(shares_valid)[1]

delta = shares_valid$shares - predict_value
MSPE <- sum((delta)^2)/n
MSPE

delta = shares_valid$shares - predict_value1
MSPE1 <- sum((delta)^2)/n
MSPE1

delta = shares_valid$shares - predict_value2
MSPE2 <- sum((delta)^2)/n
MSPE2
```



# Conclusion




# References





















